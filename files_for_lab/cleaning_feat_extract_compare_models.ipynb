{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eade243",
   "metadata": {},
   "source": [
    "# Lab | Cleaning numerical data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f95600",
   "metadata": {},
   "source": [
    "## Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983884b",
   "metadata": {},
   "source": [
    "## Load the we_fn_use_c_marketing_customer_value_analysis.csv into the variable customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df = pd.read_csv('we_fn_use_c_marketing_customer_value_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b61e4d",
   "metadata": {},
   "source": [
    "## First look at its main features (head, shape, info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(variable_customer_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304adfdf",
   "metadata": {},
   "source": [
    "## Rename the columns so they follow the PE8 (snake case) (lower_case_with_underscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df.columns\n",
    "column_names = variable_customer_df.columns\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3103b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for column in variable_customer_df.columns:\n",
    "    cols.append(column.lower().replace(' ','_'))\n",
    "cols\n",
    "variable_customer_df.columns = cols\n",
    "\n",
    "variable_customer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1600ae",
   "metadata": {},
   "source": [
    "## Change effective to date column to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87bee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['effective_to_date'] = pd.to_datetime(variable_customer_df['effective_to_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a5e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca64cb06",
   "metadata": {},
   "source": [
    "## Define a function that differentiates between continuous and discrete variables. Hint: The number of unique values might be useful. Store continuous data into a continuous_df variable and do the same for discrete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df_num = variable_customer_df.select_dtypes(include = np.number)\n",
    "variable_customer_df_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in variable_customer_df.columns:\n",
    "#     print(column, \":\", variable_customer_df[column].unique()) # unique values for each column\n",
    "    print(column, \":\", len(variable_customer_df[column].unique())) # number of unique value for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def value_counts(df):\n",
    "#     for column in df.columns:\n",
    "#         print(column, \":\", len(df[column].unique())) # number of unique value for each column\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_count(variable_customer_df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7cd24",
   "metadata": {
    "id": "wjhrEdNa6GVI"
   },
   "outputs": [],
   "source": [
    "def cont_disc(df): # df = dataframe to pass function to\n",
    "        \n",
    "    continuous_lst = [] # set up empty lists\n",
    "    discrete_lst = []\n",
    "    \n",
    "    # column becomes the next column name\n",
    "    for column in df.columns:               # < (df.shape[0] * 0.02) another option to differentiate: if the number of rows is less than cutoff it is discrete; more than cutoff, it is continuous\n",
    "        if len(df[column].unique()) >= 202: # I chose 202 as a cutoff to differentiate the continuous and discrete variables.\n",
    "            continuous_lst.append(column) #append column name to continuous\n",
    "        else:\n",
    "            discrete_lst.append(column)\n",
    "    \n",
    "    return continuous_lst, discrete_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_var, discrete_var = cont_disc(variable_customer_df_num)\n",
    "\n",
    "print('continuous=', continuous_var)\n",
    "print('discrete=', discrete_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(continuous_var)\n",
    "display(discrete_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_df = variable_customer_df_num[['customer_lifetime_value', 'income','monthly_premium_auto','total_claim_amount']]\n",
    "continuous_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ce427",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_df = variable_customer_df_num[['months_since_last_claim', 'months_since_policy_inception','number_of_open_complaints','number_of_policies']]\n",
    "discrete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d19316",
   "metadata": {
    "id": "wjhrEdNa6GVI"
   },
   "outputs": [],
   "source": [
    "# return continuous and discrete dataframes inside function\n",
    "\n",
    "# def cont_disc2(df): # df = dataframe to pass function to\n",
    "        \n",
    "#     continuous_lst = [] # set up empty lists\n",
    "#     discrete_lst = []\n",
    "    \n",
    "#     # column becomes the next column name\n",
    "#     for column in df.columns:               # < (df.shape[0] * 0.02) another option to differentiate: if the number of rows is less than cutoff it is discrete; more than cutoff, it is continuous\n",
    "#         if len(df[column].unique()) >= 202: # I chose 202 as a cutoff to differentiate the continuous and discrete variables.\n",
    "#             continuous_lst.append(column) #append column name to continuous\n",
    "#         else:\n",
    "#             discrete_lst.append(column)\n",
    "    \n",
    "#     continuous_df = df[continuous_lst]\n",
    "#     discrete_df = df[discrete_lst]\n",
    "\n",
    "#     return continuous_df, discrete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9360fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous_df, discrete_df = disc_cont2(variable_customer_df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03033945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162a871",
   "metadata": {},
   "source": [
    "## Plot a correlation matrix, comment what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3302d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = variable_customer_df.corr()\n",
    "correlations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(correlations, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no feature making high collinearity (such as 0.9%). No column was dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d663c",
   "metadata": {},
   "source": [
    "## Create a function to plot every discrete variable. Do the same with the continuous variables (Be Careful, you may need to change the plot type to one better suited for continuous data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x):\n",
    "\n",
    "    for column in x.columns:\n",
    "        sns.displot(x[column], kde=True)\n",
    "        plt.show()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f556a0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(plot(discrete_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e29b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cont(x):\n",
    "\n",
    "    for column in x.columns:\n",
    "        x[column].hist()\n",
    "        plt.show()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_cont(continuous_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a880c",
   "metadata": {},
   "source": [
    "## Comment what you can see in the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c773e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete variables: distribution plots show that month_since_last_claim and month_since_policy_inception\n",
    "# has relatively equally distributed values. Number_of_open_complaints and number_of_policies have outliers.\n",
    "\n",
    "# There are outliers in all continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53472785",
   "metadata": {},
   "source": [
    "## Look for outliers in the continuous variables. (HINT: There’s a good plot to do that!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cont(x):\n",
    "\n",
    "    for column in x.columns:\n",
    "        sns.boxplot(y=x[column])\n",
    "        plt.show()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box_cont(continuous_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(column):\n",
    "        \n",
    "    q25 = np.percentile(sorted(column),25)\n",
    "\n",
    "    q75 = np.percentile(sorted(column),75) \n",
    "        \n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    upper_limit = q75 + 1.5*iqr\n",
    "    \n",
    "    lower_limit = q25 - 1.5*iqr\n",
    "        \n",
    "    outliers = [x for x in column if x <= lower_limit or x >= upper_limit]\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2858a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tca_outliers = outliers(continuous_df['total_claim_amount'])\n",
    "tca_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_outliers = outliers(continuous_df['income'])\n",
    "income_outliers # income column has no outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb610e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_lifetime_value_outliers = outliers(continuous_df['customer_lifetime_value'])\n",
    "customer_lifetime_value_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3eae07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "monthly_premium_auto_outliers = outliers(continuous_df['monthly_premium_auto'])\n",
    "monthly_premium_auto_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6379ca",
   "metadata": {},
   "source": [
    "## Did you find outliers? Comment what you will do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although 'outliers' function determines that the columns of continuous_df except 'income' have outliers, boxes plots do not suggest\n",
    "# so many outliers in the columns of continuous_df."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fc1e0",
   "metadata": {},
   "source": [
    "## Check all columns for NaN values. Decide what (if anything) you will need to do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no null value in the variable_customer_df \n",
    "variable_customer_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dff430",
   "metadata": {},
   "source": [
    "# Lab | Cleaning categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3d678",
   "metadata": {},
   "source": [
    "## Import the necessary libraries if you are starting a new notebook. Using the same data as the previous lab: we_fn_use_c_marketing_customer_value_analysis.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same file is imported in the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd78cd7",
   "metadata": {},
   "source": [
    "## Find all of the categorical data. Save it in a categorical_df variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21baf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df = variable_customer_df.select_dtypes(include = object)\n",
    "categorical_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b423e3",
   "metadata": {},
   "source": [
    "## Check for NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd734741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no null values\n",
    "categorical_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f316afb",
   "metadata": {},
   "source": [
    "## Check all unique values of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec35715",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical_df.columns:\n",
    "    print(column, \":\", categorical_df[column].unique()) # unique values for each column\n",
    "    print(column, \":\", len(categorical_df[column].unique())) # number of unique value for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7085d",
   "metadata": {},
   "source": [
    "## Check dtypes. Do they all make sense as categorical data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4bdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df.dtypes\n",
    "\n",
    "# All columns have categorical values. This implies that the dataframe was fully categorized into numerical and\n",
    "# categorical parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4e897",
   "metadata": {},
   "source": [
    "## Does any column contain alpha and numeric data? Decide how to clean it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd61376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute NaNs with mode: this is not a great solution. In this way, we overemphasize the mode value of the column.\n",
    "# We can improve a model (KNN classifier) to predict an intermediate target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for numeric values: there is no column that has only numeric values.\n",
    "for column in categorical_df.columns:\n",
    "    print(categorical_df[column].str.isnumeric().value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca011546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for alpha values: there are some columns (i.e. state, education) that have alpha values.\n",
    "for column in categorical_df.columns: \n",
    "    print(categorical_df[column].str.isalpha().value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdeb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for alphanumeric characters: there are some columns that have alphanumeric characters.\n",
    "for column in categorical_df.columns:\n",
    "    print(categorical_df[column].str.isalnum().value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aece3c",
   "metadata": {},
   "source": [
    "## Would you choose to do anything else to clean or wrangle the categorical data? Comment your decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply qcut or cut techniques to divide the categorical data into bins.\n",
    "# For categorical variables we can plot the relation between bins and check outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2df985",
   "metadata": {},
   "source": [
    "## Compare policy_type and policy. What information is contained in these columns. Can you identify what is important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personal Auto     6788\n",
    "# Corporate Auto    1968\n",
    "# Special Auto       378\n",
    "# Name: policy_type, dtype: int64\n",
    "# Personal L3     3426\n",
    "# Personal L2     2122\n",
    "# Personal L1     1240\n",
    "# Corporate L3    1014\n",
    "# Corporate L2     595\n",
    "# Corporate L1     359\n",
    "# Special L2       164\n",
    "# Special L3       148\n",
    "# Special L1        66\n",
    "# Name: policy, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fced58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy_type column is divided into three; personal auto, corporate auto, and special auto.\n",
    "# The personal auto, corporate auto, and special auto are subdivided into three in policy column.\n",
    "# Therefore, detailed information is given in policy column.\n",
    "# In this case, policy column could be removed since all the information in policy column are also given in policy_type column as a simplified classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871ea6b",
   "metadata": {},
   "source": [
    "## Check number of unique values in each column, can they be combined in any way to ease encoding? Comment your thoughts and make those changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unique values in policy column can be combined. \n",
    "# The policy_type column already has the combined version of the unique values in policy column.\n",
    "\n",
    "# Luxury SUV and Luxury Car can be combined to form Luxury vales in vehicle_class.\n",
    "\n",
    "# Disable, retired, and medical leave can be combined and attached into Unemployed value in employment status.\n",
    "\n",
    "# Bachelor and college can be combined to form undergraduate value and Master and Doctor can be combined to form graduate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical_df.columns:\n",
    "    print(categorical_df[column].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_df = categorical_df.drop(['policy'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6612f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_df = categorical_df.replace({'vehicle_class': {'Luxury SUV': 'Luxury', 'Luxury Car': 'Luxury'}})\n",
    "\n",
    "# categorical_df = categorical_df.replace({'employmentstatus': {'Disabled': 'Unemployed', 'Retired': 'Unemployed ', 'Medical Leave': 'Unemployed'}})\n",
    "\n",
    "# categorical_df = categorical_df.replace({'education': {'Master': 'Graduate', 'Doctor': 'Graduate', 'Bachelor': 'Undergraduate','College': 'Undergraduate'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8624843",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical_df.columns:\n",
    "    print(categorical_df[column].value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621135c",
   "metadata": {},
   "source": [
    "# Lab | Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96116868",
   "metadata": {},
   "source": [
    "## Open the categoricals variable we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb021332",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = variable_customer_df.select_dtypes(np.object)\n",
    "categoricals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da7a6f",
   "metadata": {},
   "source": [
    "## Plot all the categorical variables with the proper plot. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 9134 unique variables in customer column. I do not evalute the plot of customer column.\n",
    "\n",
    "# When I look to other plots of the categorical variables, the data is imbalanced except for gender column.\n",
    "\n",
    "# Distribution of the values in the columns varies. Oversampling, undersampling or smote can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cat(df):\n",
    "    for column in df.columns:\n",
    "        sns.countplot(x=df[column])\n",
    "        plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat(categoricals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add6d74",
   "metadata": {},
   "source": [
    "## There might be some columns that seem to be redundant, check their values to be sure. What should we do with them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c00579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unique values in policy column can be combined. \n",
    "\n",
    "# The customer column has 9134 unique values. It comprises alphanumeric characters. We need to combine the unique values in customer column to decrease the number of unique values. \n",
    "\n",
    "# The policy_type column already has the combined version of the unique values in policy column.\n",
    "\n",
    "# Luxury SUV and Luxury Car can be combined to form Luxury vales in vehicle_class.\n",
    "\n",
    "# Disable, retired, and medical leave can be combined and attached into Unemployed value in employment status.\n",
    "\n",
    "# Bachelor and college can be combined to form undergraduate value and Master and Doctor can be combined to form graduate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4658053",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals.isnull().sum() # no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ce0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categoricals.columns:\n",
    "    print(column, \":\", categoricals[column].unique()) # unique values for each column\n",
    "    print(column, \":\", len(categoricals[column].unique())) # number of unique value for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0823883",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categoricals.columns:\n",
    "    print(categoricals[column].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute NaNs with mode: this is not a great solution. In this way, we overemphasize the mode value of the column.\n",
    "# We can improve a model (KNN classifier) to predict an intermediate target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for numeric values: there is no column that has only numeric values.\n",
    "for column in categoricals.columns:\n",
    "    print(categoricals[column].str.isnumeric().value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaaef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for alpha values: there are some columns (i.e. state, education) that have alpha values.\n",
    "for column in categoricals.columns: \n",
    "    print(categoricals[column].str.isalpha().value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for alphanumeric characters: there are some columns that have alphanumeric characters.\n",
    "for column in categoricals.columns:\n",
    "    print(categoricals[column].str.isalnum().value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bdcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply qcut or cut techniques to divide the categorical data into bins.\n",
    "# For categorical variables we can plot the relation between bins and check outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc20854",
   "metadata": {},
   "source": [
    "## Plot time variable. Can you extract something from it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a848d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are drops in each three or four month.\n",
    "\n",
    "# At the end or middle of certain months, the policy could expire near the payment of checks.\n",
    "\n",
    "# This might lead to drop in wages of employees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.histplot(variable_customer_df['effective_to_date'])\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad4bb6",
   "metadata": {},
   "source": [
    "# Lab | Comparing regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56592abe",
   "metadata": {},
   "source": [
    "## In this final lab, we will model our data. Import sklearn train_test_split and separate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2460659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b9ff9",
   "metadata": {},
   "source": [
    "## We will start with removing outliers, if you have not already done so. We have discussed different methods to remove outliers. Use the one you feel more comfortable with, define a function for that. Use the function to remove the outliers and apply it to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72251cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cont(x):\n",
    "\n",
    "    for column in x.columns:\n",
    "        sns.boxplot(y=x[column])\n",
    "        plt.show()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box_cont(continuous_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c655626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(column):\n",
    "        \n",
    "    q25 = np.percentile(sorted(column),25)\n",
    "\n",
    "    q75 = np.percentile(sorted(column),75) \n",
    "        \n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    upper_limit = q75 + 1.5*iqr\n",
    "    \n",
    "    lower_limit = q25 - 1.5*iqr\n",
    "        \n",
    "    outliers = [x for x in column if x <= lower_limit or x >= upper_limit]\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tca_outliers = outliers(continuous_df['total_claim_amount'])\n",
    "tca_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_outliers = outliers(continuous_df['income'])\n",
    "income_outliers # income column has no outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_lifetime_value_outliers = outliers(continuous_df['customer_lifetime_value'])\n",
    "customer_lifetime_value_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f4ac9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "monthly_premium_auto_outliers = outliers(continuous_df['monthly_premium_auto'])\n",
    "monthly_premium_auto_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'outliers' function determines that the columns of continuous_df except 'income' have outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two solutions:\n",
    "# 1. log transform is a way to deal with outliers\n",
    "# 2. remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16772f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transfom_clean(x):\n",
    "    if x>0:\n",
    "        return np.log(x)\n",
    "    else:\n",
    "        return np.NAN # We are returning NaNs so that we can replace them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02dad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plothist(col):\n",
    "    pd.Series(variable_customer_df[col].apply(log_transfom_clean)).hist()\n",
    "    plt.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5038880",
   "metadata": {},
   "outputs": [],
   "source": [
    "plothist('total_claim_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0aeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plothist('monthly_premium_auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51627c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plothist('customer_lifetime_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37386541",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['total_claim_amount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d18970",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['total_claim_amount_transformed'] = variable_customer_df['total_claim_amount'].apply(log_transfom_clean)\n",
    "\n",
    "# replace NaNs with mean of transformed data\n",
    "variable_customer_df['total_claim_amount_transformed'] = variable_customer_df['total_claim_amount_transformed'].fillna(np.mean(variable_customer_df['total_claim_amount_transformed']))\n",
    "variable_customer_df['total_claim_amount_transformed'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c890932",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['monthly_premium_auto'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9428508",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['monthly_premium_auto_transformed'] = variable_customer_df['monthly_premium_auto'].apply(log_transfom_clean)\n",
    "\n",
    "# replace NaNs with mean of transformed data\n",
    "variable_customer_df['monthly_premium_auto_transformed'] = variable_customer_df['monthly_premium_auto_transformed'].fillna(np.mean(variable_customer_df['monthly_premium_auto_transformed']))\n",
    "variable_customer_df['monthly_premium_auto_transformed'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a105d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['customer_lifetime_value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aea3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['customer_lifetime_value_transformed'] = variable_customer_df['customer_lifetime_value'].apply(log_transfom_clean)\n",
    "\n",
    "# replace NaNs with mean of transformed data\n",
    "variable_customer_df['customer_lifetime_value_transformed'] = variable_customer_df['customer_lifetime_value_transformed'].fillna(np.mean(variable_customer_df['customer_lifetime_value_transformed']))\n",
    "variable_customer_df['customer_lifetime_value_transformed'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b65bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['customer_lifetime_value_transformed'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['monthly_premium_auto_transformed'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b4ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['total_claim_amount_transformed'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df['total_claim_amount_transformed'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ul_ll(column):\n",
    "        \n",
    "    q25 = np.percentile(sorted(column),25)\n",
    "\n",
    "    q75 = np.percentile(sorted(column),75) \n",
    "        \n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    upper_limit = q75 + 1.5*iqr\n",
    "    \n",
    "    lower_limit = q25 - 1.5*iqr\n",
    "        \n",
    "    return upper_limit, lower_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_upperl_lowerl = ul_ll(variable_customer_df['income'])\n",
    "income_upperl_lowerl # income column has no outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec84d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_claim_amount_upperl_lowerl = ul_ll(variable_customer_df['total_claim_amount_transformed'])\n",
    "total_claim_amount_upperl_lowerl # income column has no outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18920a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_premium_auto_upperl_lowerl = ul_ll(variable_customer_df['monthly_premium_auto_transformed'])\n",
    "monthly_premium_auto_upperl_lowerl # income column has no outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_lifetime_value_upperl_lowerl = ul_ll(variable_customer_df['customer_lifetime_value_transformed'])\n",
    "customer_lifetime_value_upperl_lowerl # income column has no outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df_customer_lifetime_valueNaN_dropped_1 = variable_customer_df[variable_customer_df['customer_lifetime_value_transformed'] <= 10.313001003057334].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df_customer_lifetime_valueNaN_dropped_2 = variable_customer_df[variable_customer_df['customer_lifetime_value_transformed'] >= 7.0803778658926175].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0168bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df_customer_lifetime_valueNaN_dropped = pd.concat([variable_customer_df_customer_lifetime_valueNaN_dropped_1,variable_customer_df_customer_lifetime_valueNaN_dropped_2],axis=0)\n",
    "variable_customer_df_customer_lifetime_valueNaN_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df_customer_lifetime_valueNaN_dropped_1 = variable_customer_df[variable_customer_df['monthly_premium_auto_transformed'] <= 5.399108147808699].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df_customer_lifetime_valueNaN_dropped_2 = variable_customer_df[variable_customer_df['monthly_premium_auto_transformed'] >= 3.5117474395965522].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485e5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variable_customer_df_customer_lifetime_valueNaN_dropped = pd.concat([variable_customer_df_customer_lifetime_valueNaN_dropped_1,variable_customer_df_customer_lifetime_valueNaN_dropped_2],axis=0)\n",
    "variable_customer_df_customer_lifetime_valueNaN_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a94af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df_customer_lifetime_valueNaN_dropped_1 = variable_customer_df[variable_customer_df['total_claim_amount_transformed'] <= 7.353347257270782].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e613594",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df_customer_lifetime_valueNaN_dropped_2 = variable_customer_df[variable_customer_df['total_claim_amount_transformed'] >= 4.558793261718426].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e702641",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_customer_df_customer_lifetime_valueNaN_dropped = pd.concat([variable_customer_df_customer_lifetime_valueNaN_dropped_1,variable_customer_df_customer_lifetime_valueNaN_dropped_2X = pd.concat([X_normalized, onehot_encoded], axis=1) X = pd.concat([X_normalized, onehot_encoded], axis=1) ],axis=0)\n",
    "variable_customer_df_customer_lifetime_valueNaN_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ec91e",
   "metadata": {},
   "source": [
    "## Create a copy of the dataframe for the data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24862b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = variable_customer_df_customer_lifetime_valueNaN_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea17342c",
   "metadata": {},
   "source": [
    "## Normalize the continuous variables. You can use any one method you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['total_claim_amount_transformed']\n",
    "X = data.drop(['total_claim_amount_transformed'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f933cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d45175",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train.select_dtypes(include = np.number)\n",
    "X_train_cat = X_train.select_dtypes(include = object)\n",
    "X_test_num = X_test.select_dtypes(include = np.number)\n",
    "X_test_cat = X_test.select_dtypes(include = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1599f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_df = data[['customer_lifetime_value_transformed', 'income','monthly_premium_auto_transformed','total_claim_amount_transformed']]\n",
    "continuous_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61477681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991a509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MinMaxtransformer = MinMaxScaler().fit(continuous_df)\n",
    "X_normalized = MinMaxtransformer.transform(continuous_df)\n",
    "print(type(X_normalized))\n",
    "X_normalized = pd.DataFrame(X_normalized,columns=continuous_df.columns)\n",
    "display(X_normalized.head())\n",
    "print(type(X_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39da56c",
   "metadata": {},
   "source": [
    "## Encode the categorical variables (See the hint below for encoding categorical data!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d71a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_df = data[['state','coverage','employmentstatus','location_code','marital_status','policy_type',\n",
    "                    'policy','renew_offer_type','customer','months_since_last_claim', \n",
    "                    'months_since_policy_inception','number_of_open_complaints','number_of_policies',\n",
    "                   'sales_channel','vehicle_class','vehicle_size']]\n",
    "discrete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52bd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns=data[['state','marital_status','policy_type',\n",
    "                    'policy','renew_offer_type','customer','months_since_last_claim', \n",
    "                    'months_since_policy_inception','number_of_open_complaints','number_of_policies',\n",
    "                   'sales_channel','vehicle_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(drop='first').fit(onehot_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = encoder.get_feature_names(input_features=onehot_columns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233939a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoder.transform(onehot_columns).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e62f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873520d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoded = pd.DataFrame(encoded,columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = data[['coverage','employmentstatus','location_code','vehicle_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal[\"coverage\"] = ordinal[\"coverage\"].map({\"Basic\" : 0, \"Extended\" : 1, \"Premium\" : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal[\"employmentstatus\"] = ordinal[\"employmentstatus\"].map({\"Employed\" : 0, \"Unemployed\" : 1, \"Medical Leave\" : 2,\"Disabled\" : 3,\"Retired\" : 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99719b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal[\"location_code\"] = ordinal[\"location_code\"].map({\"Suburban\" : 0, \"Rural\" : 1, \"Urban\" : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ab1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal[\"vehicle_size\"] = ordinal[\"vehicle_size\"].map({\"Medsize\" : 0, \"Small\" : 1, \"Large\" : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a530836",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoded.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoded = onehot_encoded.loc[~onehot_encoded.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd933a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7188c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = ordinal.loc[~ordinal.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoded = pd.concat([onehot_encoded,ordinal],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416fd14",
   "metadata": {},
   "source": [
    "## The time variable can be useful. Try to transform its data into a useful one. Hint: Day week and month as integers might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e60b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['effective_to_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a77bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['effective_to_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['day'] = data['effective_to_date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a802c",
   "metadata": {},
   "source": [
    "## Since the model will only accept numerical data, check and make sure that every column is numerical, if some are not, change it using encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7488b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoded = onehotencoded.loc[~onehotencoded.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debe9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_df = continuous_df.loc[~continuous_df.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b71bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['year'].loc[~data['year'].index.duplicated(keep='first')]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91907250",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['month'].loc[~data['month'].index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f450c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['day'] = data['day'].loc[~data['day'].index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = data[['day','month','year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226353a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pd.concat([continuous_df, onehotencoded], axis=1)\n",
    "data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = date[~date.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new2 = pd.concat([data_new, date], axis=1)\n",
    "data_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86aae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = data_new2.select_dtypes(include = object)\n",
    "X_cat # no categoricals. all variables are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "# we use the transformer that was trained on the training data\n",
    "X_test_normalized = MinMaxtransformer.transform(X_test_num)\n",
    "X_test_norm = pd.DataFrame(X_test_normalized)\n",
    "X_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24dd120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categoricals using previous encoder\n",
    "#We do not need to fit again.\n",
    "encoded = encoder.transform(X_test_cat).toarray()\n",
    "cols = encoder.get_feature_names(input_features=X_test_cat.columns)\n",
    "onehot_encoded_test = pd.DataFrame(encoded,columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2dafc5",
   "metadata": {},
   "source": [
    "## Try a simple linear regression with all the data to see whether we are getting good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed871644",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c92ae",
   "metadata": {},
   "source": [
    "## Great! Now define a function that takes a list of models and train (and tests) them so we can try a lot of them without repeating code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018da2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81781b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e1fde2",
   "metadata": {},
   "source": [
    "## Use the function to check LinearRegressor and KNeighborsRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de660b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa71a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4f62f00",
   "metadata": {},
   "source": [
    "## You can check also the MLPRegressor for this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a0d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00acc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18de1676",
   "metadata": {},
   "source": [
    "## Check and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f28495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ff13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "becc99b2",
   "metadata": {},
   "source": [
    "# Lab | Random variable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc60d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d87fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "305c6356",
   "metadata": {},
   "source": [
    "## Get the numerical variables from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3c7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe57b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "673bae27",
   "metadata": {},
   "source": [
    "## Check using a distribution plot if the variables fit the theoretical normal or exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b77fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba018ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09752413",
   "metadata": {},
   "source": [
    "## Check if any of the transformations (log-transform, etc.) we have seen up to this point changes the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84566937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "200px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
